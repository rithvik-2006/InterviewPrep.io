This project, titled "Optimizing Query Generation for Enhanced Document Retrieval in RAG," was conducted by Hamin Koo, Minseon Kim, and Sung Ju Hwang.

Here is a detailed, topic-wise summary of the project:

*   **Introduction and Problem Statement**
    *   Large Language Models (LLMs) are highly capable in various language tasks but frequently generate incorrect information, a problem known as "hallucinations". These hallucinations significantly undermine LLMs' reliability.
    *   Retrieval-Augmented Generation (RAG) systems aim to mitigate hallucinations by leveraging document retrieval to ground LLM responses in factual information from retrieved documents, thereby providing more accurate answers to user queries.
    *   However, RAG systems can still suffer from hallucinations, particularly when faced with vague queries that do not accurately capture the user's intent. Short or ambiguous queries negatively impact search results, highlighting a significant limitation of RAG in LLMs.

*   **Proposed Solution: Query Optimization using Query expAnsion (QOQA)**
    *   The primary goal of this study is to generate **concrete and precise queries** for document retrieval within RAG systems by optimizing the query itself.
    *   The authors propose **Query Optimization using Query expAnsion (QOQA)** to achieve this. QOQA refines queries using LLMs based on a top-k averaged query-document alignment score.
    *   This approach aims to be computationally efficient, improve the precision of document retrieval, and consequently reduce hallucinations. Experiments showed an average accuracy gain of 1.6% in document retrieval.

*   **Methodology and Key Components of QOQA**
    *   **Query Optimization with LLM**:
        *   QOQA uses an LLM to rephrase the query based on its alignment score.
        *   Initially, the original query is used to retrieve N documents.
        *   The original query is then concatenated with these top N retrieved documents to create an **expanded query**.
        *   This expanded query is sent to the LLM to generate R0 rephrased queries.
        *   These rephrased queries are then evaluated for their alignment with the retrieved documents using a retrieval model.
        *   The pairs of query-document alignment scores and queries are stored in a "query bucket".
        *   In subsequent optimization steps, the prompt template is updated with the original query, retrieved documents, and the top K rephrased queries from the query bucket. The original query information is always included to ensure improved performance. The LLM generates Ri rephrased queries based on these scores and adds them to the query bucket.
    *   **Query-document Alignment Score**:
        *   To facilitate query optimization, QOQA employs three types of evaluation scores to measure the correlation between a query and retrieved documents:
            *   **BM25 Score (Sparse Retrieval)**: Calculated based on the frequency of query terms in a document, document length, and inverse document frequency (IDF) of query terms.
            *   **Dense Score (Dense Retrieval)**: Represents the relevance score between queries and documents using learned dense representations (embedding vectors). It's calculated as the dot product of the query's embedding vector and the document's embedding vector. The BAAI/bge-base-en-v1.5 model was used for this.
            *   **Hybrid Score**: Combines both BM25 and Dense scores with a tuning parameter alpha (α).
    *   **Prompt Template**: The prompt template used in QOQA includes instructions for the optimizing task, the original query with top-N retrieved documents, and revised queries generated by the LLM optimizer along with their scores.
    *   **Implementation Details**:
        *   GPT-3.5-Turbo was used as the LLM optimizer.
        *   The temperature was set to 1.0.
        *   Maximum optimization iteration was set to 50 (i = 1 to 50).
        *   Specific parameters used were N=5 (top retrieved documents), K=3 (top rephrased queries in prompt), R0=3 (initial rephrased queries), and Ri=1 (subsequent rephrased queries).
        *   Hyperparameters for BM25 (k1=1.2, b=0.75) and Hybrid (α=0.1) were set to default Pyserini values.

*   **Evaluation and Results**
    *   **Datasets**: The project was evaluated on three retrieval datasets from BEIR: SciFact (for fact-checking scientific claims), Trec-Covid (for biomedical information retrieval), and FiQA (for financial question answering).
    *   **Baselines**:
        *   **Sparse Retrieval Baselines**: Included BM25, BM25+RM3 (Pseudo Relevance Feedback), BM25+Q2D/PRF (LLM-based and PRF query expansion), and CSQE (Corpus-Steered Query Expansion).
        *   **Dense Retrieval Baselines**: Included BGE-base-en-v1.5 and BGE-base-en-v1.5+CSQE.
    *   **Performance Metric**: All scores reported denote nDCG@10 (normalized Discounted Cumulative Gain at 10).
    *   **Key Findings**:
        *   QOQA consistently showed performance gains across different datasets in improving retrievals.
        *   For **dense retrieval**, QOQA variants exhibited superior performance. **QOQA (BM25 score) achieved the best result in SciFact with 75.4**, and QOQA (Hybrid score) showed strong performance in Trec-Covid with 79.2.
        *   For **sparse retrieval**, QOQA (Dense score) achieved 69.7 on SciFact and QOQA (BM25 score) achieved 61.1 on Trec-Covid.
    *   **Case Analysis**: Rephrased queries generated by QOQA were found to be **more precise and concrete** than original queries. Examples demonstrated that QOQA-generated queries include precise keywords (e.g., "nano-sized" or "molecular evidence"), leading to better alignment and retrieval of relevant documents containing correct answers.
    *   **Ablation Studies**:
        *   The study systematically removed the expansion and optimization components from QOQA to assess their impact on nDCG@10 results.
        *   Removing the **expansion component** (using retrieved documents to expand the query) led to a **significant performance drop**, especially with the BM25 score, highlighting its critical role in generating high-quality rephrased queries.
        *   The **optimization step** (iterative refinement of queries based on scores) was also shown to improve the search for better rephrased queries.

*   **Conclusion and Future Work**
    *   The project successfully addressed the issue of hallucinations in RAG systems by optimizing query generation.
    *   By refining queries using LLMs and a top-k averaged query-document alignment score, **precision and computational efficiency in document retrieval were improved**, leading to a 1.6% average gain in accuracy and reduced hallucinations.
    *   This study underscores the importance of precise query generation for enhancing the dependability and effectiveness of RAG systems.
    *   Future work will focus on integrating more advanced query refinement techniques and applying the QOQA approach to a broader range of RAG applications.